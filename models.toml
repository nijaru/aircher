# Aircher Model Registry
# This file can be updated independently of code releases
# Last updated: 2025-01-16

[meta]
version = "2025.01.16"
last_updated = "2025-01-16T00:00:00Z"
source = "Community maintained - update as new models are released"

# Model quality tiers and capabilities
# Update this section when new SOTA models are released

[quality_tiers]

[quality_tiers.flagship]
description = "Current state-of-the-art models for critical tasks"
models = [
    "claude-sonnet-4",           # Latest Claude SOTA (if available)
    "claude-opus-4",             # Latest Claude SOTA (if available) 
    "claude-3-5-sonnet-20241022", # Current best Claude
    "gpt-4o",                    # Current best OpenAI
    "gemini-2.5-pro",            # Latest Gemini SOTA (if available)
    "grok-4",                    # Latest xAI SOTA (if available)
    "kimi-k2",                   # Latest Moonshot SOTA (if available)
]
use_for = ["code_review", "debugging", "refactoring", "code_generation", "architecture_review"]

[quality_tiers.premium]
description = "High-quality models for most tasks"
models = [
    "claude-3-5-haiku-20241022",
    "gpt-4o-mini", 
    "gemini-2.0-flash-exp",
    "llama3.3",                  # Best Ollama option
]
use_for = ["documentation", "general"]

[quality_tiers.standard]
description = "Cost-effective models for routine tasks"
models = [
    "gpt-4o-mini",
    "claude-3-5-haiku-20241022",
    "gemini-1.5-flash",
    "llama3.1",
]
use_for = ["summaries", "quick_questions"]

[quality_tiers.basic]
description = "Cheapest models for simple tasks"
models = [
    "gpt-4o-mini",              # Often cheaper than gpt-3.5-turbo!
    "gemini-1.5-flash",
    "llama3.1",
]
use_for = ["commit_messages"]

# Provider-specific model information
# Add new providers/models here as they become available

[providers.anthropic]
flagship_models = ["claude-sonnet-4", "claude-opus-4", "claude-3-5-sonnet-20241022"]
coding_specialist = "claude-3-5-sonnet-20241022"  # Best for code
reasoning_specialist = "claude-3-5-sonnet-20241022"
cost_effective = "claude-3-5-haiku-20241022"

[providers.openai]
flagship_models = ["gpt-4o", "o1-preview"]
coding_specialist = "gpt-4o"
reasoning_specialist = "o1-preview"
cost_effective = "gpt-4o-mini"

[providers.google]
flagship_models = ["gemini-2.5-pro", "gemini-2.0-flash-exp"]
coding_specialist = "gemini-2.0-flash-exp"
reasoning_specialist = "gemini-2.5-pro"
cost_effective = "gemini-1.5-flash"

[providers.xai]
flagship_models = ["grok-4", "grok-3"]
coding_specialist = "grok-4"
reasoning_specialist = "grok-4"
cost_effective = "grok-3"
notes = "xAI models - may need separate integration"

[providers.moonshot]
flagship_models = ["kimi-k2", "kimi-k1"]
coding_specialist = "kimi-k2"
reasoning_specialist = "kimi-k2"
cost_effective = "kimi-k1"
notes = "Moonshot AI models - may need separate integration"

[providers.ollama]
flagship_models = ["llama3.3", "codellama", "qwen2.5"]
coding_specialist = "codellama"
reasoning_specialist = "llama3.3"
cost_effective = "llama3.1"
notes = "Free local models"

# Task-specific recommendations
# These can be updated as new models prove better for specific tasks

[task_recommendations]

[task_recommendations.code_review]
description = "Critical task - always use SOTA models"
minimum_tier = "flagship"
preferred_models = [
    "claude-sonnet-4",           # If available
    "claude-3-5-sonnet-20241022", # Current best
    "gpt-4o",
    "gemini-2.5-pro",           # If available
]
fallback_free = ["codellama", "llama3.3"]

[task_recommendations.debugging]
description = "Critical reasoning task"
minimum_tier = "flagship"  
preferred_models = [
    "claude-sonnet-4",
    "o1-preview",               # Strong reasoning
    "gpt-4o",
    "gemini-2.5-pro",
]
fallback_free = ["llama3.3", "codellama"]

[task_recommendations.code_generation]
description = "Must produce correct code"
minimum_tier = "flagship"
preferred_models = [
    "claude-sonnet-4",
    "gpt-4o", 
    "codellama",                # Specialized for code
    "kimi-k2",                  # If available
]
fallback_free = ["codellama", "llama3.3"]

[task_recommendations.refactoring]
description = "Architectural understanding required"
minimum_tier = "flagship"
preferred_models = [
    "claude-sonnet-4",
    "claude-3-5-sonnet-20241022",
    "gpt-4o",
]
fallback_free = ["llama3.3"]

[task_recommendations.architecture_review]
description = "Business-critical decisions"
minimum_tier = "flagship"
preferred_models = [
    "claude-sonnet-4",
    "claude-3-5-sonnet-20241022",
    "o1-preview",
    "gpt-4o",
]
fallback_free = ["llama3.3"]

[task_recommendations.quick_questions]
description = "Cost-efficient for simple Q&A"
minimum_tier = "standard"
preferred_models = [
    "gpt-4o-mini",              # Best cost/performance
    "claude-3-5-haiku-20241022",
    "gemini-1.5-flash",
]
fallback_free = ["llama3.1", "phi3"]

[task_recommendations.commit_messages]
description = "Simple text generation"
minimum_tier = "basic"
preferred_models = [
    "gpt-4o-mini",              # Surprisingly good and cheap
    "gemini-1.5-flash",
    "claude-3-5-haiku-20241022",
]
fallback_free = ["llama3.1", "mistral"]

[task_recommendations.summaries]
description = "Good comprehension needed"
minimum_tier = "standard"
preferred_models = [
    "claude-3-5-haiku-20241022", # Excellent for summaries
    "gpt-4o-mini",
    "gemini-1.5-flash",
]
fallback_free = ["llama3.1", "qwen2.5"]

[task_recommendations.documentation]
description = "Clear communication important"
minimum_tier = "premium"
preferred_models = [
    "claude-3-5-haiku-20241022", # Great writing
    "gpt-4o-mini",
    "gemini-2.0-flash-exp",
]
fallback_free = ["llama3.3", "qwen2.5"]

# Pricing hints (may be outdated - should fetch real-time when possible)
[pricing_hints]
notes = "These are rough estimates - always fetch real-time pricing when possible"

[pricing_hints.openai]
"gpt-4o" = { input_per_1m = 5.0, output_per_1m = 15.0 }
"gpt-4o-mini" = { input_per_1m = 0.15, output_per_1m = 0.6 }
"gpt-3.5-turbo" = { input_per_1m = 0.5, output_per_1m = 1.5 }  # Often more expensive than gpt-4o-mini!

[pricing_hints.anthropic]
"claude-3-5-sonnet-20241022" = { input_per_1m = 3.0, output_per_1m = 15.0 }
"claude-3-5-haiku-20241022" = { input_per_1m = 0.25, output_per_1m = 1.25 }

[pricing_hints.google]
"gemini-2.0-flash-exp" = { input_per_1m = 0.075, output_per_1m = 0.30 }
"gemini-1.5-flash" = { input_per_1m = 0.075, output_per_1m = 0.30 }

[pricing_hints.ollama]
"*" = { input_per_1m = 0.0, output_per_1m = 0.0 }  # Free local models