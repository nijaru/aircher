# Agent Reasoning Strategies Configuration
# Based on industry research and best practices from leading AI agents

strategies:
  # ReAct: Reasoning + Acting (from Google Research)
  # Best for: Tasks requiring interleaved reasoning and tool use
  react:
    name: "ReAct"
    description: "Synergizing Reasoning and Acting - interleave thought, action, observation"
    phases:
      - name: "Thought"
        description: "Reason about the current state and what to do next"
        actions:
          - type: "analyze"
            description: "Analyze current situation and context"
          - type: "plan"
            description: "Determine next action based on observations"
        success_criteria:
          - "Clear understanding of current state"
          - "Identified next logical step"

      - name: "Action"
        description: "Execute a tool or operation"
        actions:
          - type: "execute"
            description: "Run the selected tool with parameters"
        success_criteria:
          - "Tool executed successfully"
          - "Result obtained"

      - name: "Observation"
        description: "Observe and interpret the results"
        actions:
          - type: "observe"
            description: "Analyze tool output and update understanding"
        success_criteria:
          - "Results interpreted correctly"
          - "Context updated with new information"

    loop_until:
      - "Task completed"
      - "Max iterations reached"
      - "Blocked and needs help"

  # Reflexion: Self-Reflection for improvement (from Shinn et al)
  # Best for: Tasks that benefit from iterative refinement
  reflexion:
    name: "Reflexion"
    description: "Language agents with verbal reinforcement learning through self-reflection"
    phases:
      - name: "Initial Attempt"
        description: "First attempt at solving the task"
        actions:
          - type: "execute"
            description: "Attempt the task with current understanding"
        success_criteria:
          - "Attempt completed (success or failure)"

      - name: "Evaluation"
        description: "Evaluate the attempt's success"
        actions:
          - type: "test"
            description: "Run tests or validation checks"
          - type: "score"
            description: "Assess quality of solution"
        success_criteria:
          - "Performance metrics obtained"
          - "Success/failure determined"

      - name: "Self-Reflection"
        description: "Generate verbal feedback on what went wrong"
        actions:
          - type: "analyze_failure"
            description: "Identify why the attempt failed"
          - type: "generate_insights"
            description: "Create actionable feedback for improvement"
        success_criteria:
          - "Clear understanding of failures"
          - "Specific improvement suggestions generated"

      - name: "Memory Update"
        description: "Store reflections for next attempt"
        actions:
          - type: "store_reflection"
            description: "Add insights to episodic memory"
        success_criteria:
          - "Reflections stored"
          - "Memory updated"

      - name: "Retry with Reflection"
        description: "New attempt incorporating learned insights"
        actions:
          - type: "execute_improved"
            description: "Retry with memory of past failures"
        success_criteria:
          - "Improvement over previous attempt"

    max_iterations: 3

  # Tree of Thoughts: Explore multiple reasoning paths
  # Best for: Complex problems with multiple solution paths
  tree_of_thoughts:
    name: "Tree of Thoughts"
    description: "Deliberate problem solving by exploring multiple reasoning paths"
    phases:
      - name: "Problem Decomposition"
        description: "Break down the problem into thought steps"
        actions:
          - type: "decompose"
            description: "Identify intermediate steps needed"
        success_criteria:
          - "Problem broken into manageable steps"

      - name: "Thought Generation"
        description: "Generate multiple candidate thoughts"
        actions:
          - type: "generate_candidates"
            description: "Create multiple possible next steps"
            parameters:
              beam_width: 5  # Number of thoughts to maintain
        success_criteria:
          - "Multiple diverse thoughts generated"

      - name: "State Evaluation"
        description: "Evaluate and score each thought"
        actions:
          - type: "evaluate"
            description: "Score each thought's promise"
          - type: "prune"
            description: "Remove unpromising branches"
        success_criteria:
          - "Thoughts scored and ranked"
          - "Best paths identified"

      - name: "Search"
        description: "Navigate the thought tree"
        actions:
          - type: "explore"
            description: "Follow most promising path"
          - type: "backtrack"
            description: "Return to previous state if stuck"
        success_criteria:
          - "Solution found or all paths exhausted"

    search_algorithm: "beam_search"  # or "breadth_first", "best_first"

  # Anthropic's Workflow Pattern: Orchestrated tool use
  # Best for: Well-defined tasks with predictable steps
  workflow_orchestration:
    name: "Workflow Orchestration"
    description: "LLMs and tools orchestrated through predefined code paths"
    phases:
      - name: "Task Classification"
        description: "Identify task type and requirements"
        actions:
          - type: "classify"
            description: "Determine task category"
          - type: "route"
            description: "Select appropriate workflow"
        success_criteria:
          - "Task type identified"
          - "Workflow selected"

      - name: "Parallel Execution"
        description: "Execute independent subtasks in parallel"
        actions:
          - type: "section"
            description: "Break into parallel subtasks"
          - type: "execute_parallel"
            description: "Run subtasks simultaneously"
        success_criteria:
          - "All subtasks completed"

      - name: "Synthesis"
        description: "Combine results from parallel tasks"
        actions:
          - type: "aggregate"
            description: "Merge subtask results"
          - type: "validate"
            description: "Ensure consistency"
        success_criteria:
          - "Results successfully combined"
          - "Output validated"

  # Devin's Interactive Planning: Agent-native development
  # Best for: Complex software engineering tasks
  interactive_planning:
    name: "Interactive Planning"
    description: "Proactive research and detailed planning with human collaboration"
    phases:
      - name: "Codebase Research"
        description: "Deep exploration of existing code"
        actions:
          - type: "index"
            description: "Index repository and build knowledge graph"
          - type: "analyze_architecture"
            description: "Understand system design and patterns"
          - type: "identify_dependencies"
            description: "Map component relationships"
        success_criteria:
          - "Codebase structure understood"
          - "Key files and patterns identified"

      - name: "Plan Development"
        description: "Create detailed implementation plan"
        actions:
          - type: "generate_plan"
            description: "Create step-by-step plan"
          - type: "identify_risks"
            description: "Find potential issues"
          - type: "estimate_complexity"
            description: "Assess task difficulty"
        success_criteria:
          - "Comprehensive plan created"
          - "Risks identified"

      - name: "Human Review"
        description: "Get human feedback on plan"
        actions:
          - type: "present_plan"
            description: "Show plan to human"
          - type: "incorporate_feedback"
            description: "Adjust based on input"
        success_criteria:
          - "Human approves plan"

      - name: "Parallel Execution"
        description: "Execute plan with multiple parallel agents"
        actions:
          - type: "spawn_workers"
            description: "Create worker agents for subtasks"
          - type: "coordinate"
            description: "Manage dependencies between workers"
        success_criteria:
          - "All subtasks completed"
          - "No conflicts"

      - name: "Continuous Testing"
        description: "Test throughout implementation"
        actions:
          - type: "run_tests"
            description: "Execute test suite"
          - type: "fix_failures"
            description: "Address test failures immediately"
        success_criteria:
          - "All tests passing"

  # SWE-bench Optimized: For repository-level code changes
  swe_bench_strategy:
    name: "SWE-bench Strategy"
    description: "Optimized for solving real GitHub issues"
    phases:
      - name: "Issue Analysis"
        description: "Understand the problem deeply"
        actions:
          - type: "parse_issue"
            description: "Extract key requirements"
          - type: "reproduce_bug"
            description: "Recreate the issue locally"
        success_criteria:
          - "Issue reproduced"
          - "Root cause identified"

      - name: "Codebase Exploration"
        description: "Find relevant code locations"
        actions:
          - type: "search_symbols"
            description: "Find classes, functions, variables"
          - type: "trace_execution"
            description: "Follow code execution path"
          - type: "analyze_tests"
            description: "Understand test coverage"
        success_criteria:
          - "Relevant files identified"
          - "Code flow understood"

      - name: "Solution Design"
        description: "Design the fix approach"
        actions:
          - type: "design_solution"
            description: "Plan code changes"
          - type: "consider_alternatives"
            description: "Evaluate different approaches"
        success_criteria:
          - "Solution approach defined"
          - "Edge cases considered"

      - name: "Implementation"
        description: "Make the actual code changes"
        actions:
          - type: "edit_files"
            description: "Modify code files"
          - type: "add_tests"
            description: "Write test cases"
        success_criteria:
          - "Code changes complete"
          - "Tests added"

      - name: "Validation"
        description: "Ensure fix works and doesn't break anything"
        actions:
          - type: "run_specific_tests"
            description: "Run tests for changed code"
          - type: "run_full_suite"
            description: "Run entire test suite"
          - type: "verify_fix"
            description: "Confirm issue is resolved"
        success_criteria:
          - "All tests passing"
          - "Issue confirmed fixed"
          - "No regressions"

# Strategy Selection Rules
selection_rules:
  - condition: "task_type == 'debugging' or task_type == 'bug_fix'"
    strategy: "swe_bench_strategy"
    reason: "Optimized for finding and fixing bugs in codebases"

  - condition: "task_type == 'exploration' or task_type == 'understanding'"
    strategy: "react"
    reason: "Good for iterative exploration with tool use"

  - condition: "task_type == 'complex_reasoning' or task_type == 'puzzle'"
    strategy: "tree_of_thoughts"
    reason: "Best for problems requiring exploration of multiple paths"

  - condition: "task_type == 'code_generation' and complexity == 'high'"
    strategy: "interactive_planning"
    reason: "Complex code needs detailed planning"

  - condition: "task_type == 'refactoring' or needs_improvement == true"
    strategy: "reflexion"
    reason: "Iterative improvement through self-reflection"

  - condition: "task_type == 'workflow' and subtasks_known == true"
    strategy: "workflow_orchestration"
    reason: "Efficient for well-defined multi-step tasks"

  - condition: "default"
    strategy: "react"
    reason: "ReAct is a good general-purpose strategy"

# Performance Benchmarks (from research)
benchmarks:
  react:
    hotpot_qa: 0.35  # 35% success rate
    alfworld: 0.71   # 71% success rate

  reflexion:
    human_eval: 0.88  # 88% pass@1
    alfworld: 0.97    # 130/134 tasks

  tree_of_thoughts:
    game_of_24: 0.74  # 74% success rate
    improvement_over_cot: 1.25  # 25% better than chain-of-thought

  devin:
    swe_bench: 0.14   # 13.86% originally

  claude_opus_4:
    swe_bench: 0.72   # 72.5% latest