# Aircher Cost-Optimized Configuration Example
# This configuration demonstrates intelligent model selection for different tasks
# to minimize costs while maintaining quality where it matters most.

[providers]
default = "auto"                          # Let Aircher choose the best available provider
fallback_order = ["openai", "claude", "gemini", "ollama"]
health_check_interval = "5m"

[providers.openai]
api_key_env = "OPENAI_API_KEY"
model = "gpt-4"                          # Default for complex tasks
max_tokens = 4096
temperature = 0.7
requests_per_min = 60
base_url = "https://api.openai.com/v1"

[providers.claude]
api_key_env = "ANTHROPIC_API_KEY"
model = "claude-3-5-sonnet"              # Default for reasoning tasks
max_tokens = 4096
temperature = 0.7
requests_per_min = 50

[providers.gemini]
api_key_env = "GEMINI_API_KEY"
project = "your-project-id"
model = "gemini-pro"
max_tokens = 2048
temperature = 0.7
location = "us-central1"

[providers.ollama]
# Auto-discovery enabled by default (empty base_url)
base_url = ""
# Optional: Configure fallback URLs for faster discovery
fallback_urls = [
    "http://localhost:11434",
    "http://100.64.0.1:11434",     # Common Tailscale IP
]
model = "llama3.3"                       # Latest free local model
temperature = 0.7

# Task-specific model overrides for cost optimization
# These selections can save 60-90% on LLM costs compared to using GPT-4 for everything
[models.tasks]

# Ultra-cheap tasks (~$0.001/1K tokens)
commit_messages = "gpt-3.5-turbo"        # Simple text generation - 90% cost savings
quick_questions = "gpt-3.5-turbo"        # Fast Q&A responses - 85% cost savings
summaries = "claude-3-haiku"             # Excellent summarization - 95% cost savings

# Balanced cost/quality tasks (~$0.003-0.015/1K tokens)
documentation = "claude-3-haiku"         # Good writing quality - 90% cost savings
code_review = "claude-3-5-sonnet"        # Strong analysis - 50% cost savings vs GPT-4
debugging = "claude-3-5-sonnet"          # Excellent reasoning - 50% cost savings

# High-quality tasks (keep premium models)
refactoring = "gpt-4"                    # Complex architectural decisions
code_generation = "gpt-4"                # High-quality code output
architecture_review = "gpt-4"            # Critical design decisions

[models]
auto_select = true                       # Enable intelligent model selection
prefer_cost_efficient = true            # Prioritize cost over premium models
fallback_to_cheaper = true              # Auto-downgrade if budget limits hit

# Cost controls and budgets
[costs]
monthly_budget = 30.0                    # Conservative $30/month budget
daily_limit = 2.0                        # Prevent surprise bills
alert_threshold = 0.75                   # Warning at 75% of budget
track_usage = true                       # Enable detailed tracking
auto_downgrade_on_limit = true           # Switch to cheaper models near limits

# Per-provider daily limits
[costs.limits]
openai_daily = 1.5                       # Max $1.50/day on OpenAI
claude_daily = 1.0                       # Max $1.00/day on Claude
gemini_daily = 0.5                       # Max $0.50/day on Gemini
ollama_daily = 0.0                       # Local models are free

# Task-specific budget allocations (optional)
[costs.task_budgets]
commit_messages = 1.0                    # Max $1/month for commits
summaries = 2.0                          # Max $2/month for summaries
quick_questions = 8.0                    # Max $8/month for Q&A
code_review = 12.0                       # Max $12/month for reviews
documentation = 3.0                      # Max $3/month for docs
debugging = 10.0                         # Max $10/month for debugging

# Interface preferences
[interface]
show_thinking = true                     # Show AI reasoning process
show_context_usage = true               # Display token usage (e.g. "2.1k/8k")
show_cost_estimate = true               # Show estimated cost per request
streaming = true                        # Real-time response streaming
confirm_expensive_requests = true       # Ask before high-cost operations

# Context management
[context]
max_files = 15                          # Limit context size to control costs
auto_compaction = true                  # Optimize conversations automatically
smart_file_selection = true             # Use relevance scoring

# Development workflow optimizations
[workflow]
auto_commit_messages = true             # Generate commits with cheap model
review_before_push = true               # Automatic code review
daily_summary = true                    # End-of-day summary with cheap model

# Example cost projections with this configuration:
#
# Typical developer usage:
# - 20 commit messages/day: $0.02/month (vs $2.00 with GPT-4)
# - 10 quick questions/day: $0.30/month (vs $3.00 with GPT-4)
# - 5 code reviews/week: $2.50/month (vs $5.00 with GPT-4)
# - 3 debugging sessions/week: $1.80/month (vs $3.60 with GPT-4)
# - Daily summaries: $0.15/month (vs $1.50 with GPT-4)
#
# Total estimated monthly cost: ~$8-12 (vs $40-60 with premium models only)
# Savings: 70-80% cost reduction with minimal quality impact
