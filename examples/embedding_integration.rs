//! Example of integrating the embedding system with Aircher's AI code agent
//! 
//! This demonstrates:
//! - Auto-detecting and setting up embedding models
//! - Using embeddings for semantic code search
//! - Integrating with the cost tracking system
//! - Providing fallbacks when embeddings aren't available

use anyhow::Result;
use aircher::cost::{EmbeddingManager, EmbeddingConfig, ModelConfiguration, ModelTier};
use aircher::commands::quick_embedding_setup;

#[tokio::main]
async fn main() -> Result<()> {
    tracing_subscriber::init();
    
    println!("ü§ñ Aircher AI Code Agent - Embedding Integration Demo\n");
    
    // 1. Quick setup for new users (answers user's question about prompting)
    demonstrate_setup_strategies().await?;
    
    // 2. Show integration with model configuration
    demonstrate_model_integration().await?;
    
    // 3. Show practical usage in AI coding scenarios
    demonstrate_coding_scenarios().await?;
    
    Ok(())
}

/// Demonstrates different setup strategies based on user preferences
async fn demonstrate_setup_strategies() -> Result<()> {
    println!("üìã Embedding Setup Strategies:\n");
    
    // Strategy 1: Auto-setup (minimal friction)
    println!("1Ô∏è‚É£ Auto-Setup (Recommended for most users):");
    let embedding_model = quick_embedding_setup().await?;
    match embedding_model {
        Some(model) => println!("   ‚úÖ Auto-selected: {}", model),
        None => println!("   ‚ö†Ô∏è  No embedding model available"),
    }
    println!();
    
    // Strategy 2: Configurable with sensible defaults
    println!("2Ô∏è‚É£ Configurable Setup:");
    let config = EmbeddingConfig {
        preferred_model: "nomic-embed-text".to_string(), // Best for AI coding
        auto_download: true, // Don't prompt by default
        use_ollama_if_available: true,
        max_model_size_mb: 700, // Allow up to mxbai-embed-large
        ..Default::default()
    };
    
    let mut manager = EmbeddingManager::new(config);
    match manager.auto_select_model().await? {
        Some(model) => {
            println!("   ‚úÖ Selected: {} ({}MB)", model.name, model.size_mb);
            println!("   üìä Optimized for: {}", model.optimized_for.join(", "));
        }
        None => println!("   ‚ö†Ô∏è  Setup required"),
    }
    println!();
    
    // Strategy 3: Show what interactive would look like
    println!("3Ô∏è‚É£ Interactive Setup (for power users):");
    println!("   Would prompt with options:");
    println!("   ‚Ä¢ Default: nomic-embed-text (274MB) - balanced");
    println!("   ‚Ä¢ Large: mxbai-embed-large (669MB) - highest quality");
    println!("   ‚Ä¢ Skip: Continue without embeddings");
    println!();
    
    Ok(())
}

/// Show how embedding models integrate with the 3-tier model system
async fn demonstrate_model_integration() -> Result<()> {
    println!("üîó Integration with Model Configuration:\n");
    
    let model_config = ModelConfiguration::default();
    let embedding_config = EmbeddingConfig::default();
    let mut embedding_manager = EmbeddingManager::new(embedding_config);
    
    // Show how embeddings enhance each tier
    println!("Integration with 3-tier system:");
    
    if let Some(claude_models) = model_config.get_provider_models("claude") {
        println!("üìç Claude Provider:");
        println!("   Planning: {} + semantic analysis", claude_models.planning);
        println!("   Main: {} + code search", claude_models.main);
        println!("   Light: {} + quick lookup", claude_models.light);
    }
    
    if let Some(ollama_models) = model_config.get_provider_models("ollama") {
        println!("üìç Ollama Provider:");
        println!("   Planning: {} + nomic-embed-text", ollama_models.planning);
        println!("   Main: {} + nomic-embed-text", ollama_models.main);
        println!("   Light: {} + basic search", ollama_models.light);
        if let Some(ref embedding) = ollama_models.embedding {
            println!("   Embedding: {} ‚úÖ", embedding);
        }
    }
    
    // Show cost implications
    println!("\nüí∞ Cost Impact:");
    println!("   ‚Ä¢ Ollama models + embeddings: Free");
    println!("   ‚Ä¢ Cloud models: Enhanced context for better results");
    println!("   ‚Ä¢ Hybrid: Use embeddings to pre-filter, then cloud models");
    println!();
    
    Ok(())
}

/// Demonstrate practical usage scenarios for AI coding
async fn demonstrate_coding_scenarios() -> Result<()> {
    println!("üõ†Ô∏è AI Coding Scenarios with Embeddings:\n");
    
    let config = EmbeddingConfig::default();
    let mut manager = EmbeddingManager::new(config);
    
    match manager.get_recommended_model().await {
        Ok(model) => {
            println!("Using embedding model: {} for the following scenarios:\n", model.name);
            
            // Scenario 1: Code search and context
            println!("1Ô∏è‚É£ Semantic Code Search:");
            println!("   User: 'Find functions that handle user authentication'");
            println!("   Process:");
            println!("   ‚Ä¢ Use {} to embed query", model.name);
            println!("   ‚Ä¢ Search codebase for similar function embeddings");
            println!("   ‚Ä¢ Provide relevant context to main model");
            println!("   ‚Ä¢ Main model gives focused, contextual response");
            println!();
            
            // Scenario 2: Architecture analysis
            println!("2Ô∏è‚É£ Architecture Analysis:");
            println!("   User: 'How do these components interact?'");
            println!("   Process:");
            println!("   ‚Ä¢ Embed code structure and relationships");
            println!("   ‚Ä¢ Find related patterns across files");
            println!("   ‚Ä¢ Use planning model with rich context");
            println!("   ‚Ä¢ Generate comprehensive architecture insights");
            println!();
            
            // Scenario 3: Bug hunting
            println!("3Ô∏è‚É£ Bug Pattern Detection:");
            println!("   User: 'Find potential null pointer issues'");
            println!("   Process:");
            println!("   ‚Ä¢ Embed known bug patterns");
            println!("   ‚Ä¢ Scan codebase for similar patterns");
            println!("   ‚Ä¢ Use main model to analyze findings");
            println!("   ‚Ä¢ Suggest fixes with context");
            println!();
            
            // Show model selection logic
            println!("üéØ Model Selection Logic:");
            println!("   ‚Ä¢ Light tasks: Basic keyword search + light model");
            println!("   ‚Ä¢ Main tasks: Embedding search + main model");
            println!("   ‚Ä¢ Planning tasks: Deep embedding analysis + planning model");
            println!();
            
        }
        Err(_) => {
            println!("‚ùå No embedding model available");
            println!("AI coding will use text-based analysis only");
            println!("Install Ollama for enhanced semantic capabilities");
        }
    }
    
    // Answer the user's question about which is best for AI agent coding
    println!("üèÜ Best Models for AI Agent Coding:\n");
    println!("For AI agents specifically:");
    println!("   1. nomic-embed-text (274MB) - Designed for code");
    println!("      ‚Ä¢ Optimized for function similarity");
    println!("      ‚Ä¢ Excellent code/documentation boundary detection");
    println!("      ‚Ä¢ Fast inference for real-time suggestions");
    println!();
    println!("   2. mxbai-embed-large (669MB) - Highest quality");
    println!("      ‚Ä¢ Best for complex architectural analysis");
    println!("      ‚Ä¢ Superior cross-language understanding");
    println!("      ‚Ä¢ Worth the size for professional development");
    println!();
    println!("   3. bge-m3 (1.2GB) - For multilingual codebases");
    println!("      ‚Ä¢ Best if working with mixed languages");
    println!("      ‚Ä¢ Excellent documentation understanding");
    println!("      ‚Ä¢ Good for international team codebases");
    println!();
    
    println!("üí° Recommendation: Start with nomic-embed-text, upgrade to mxbai-embed-large if needed");
    
    Ok(())
}