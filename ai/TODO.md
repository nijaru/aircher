# TODO

## Current Sprint: Week 9 - Empirical Validation & Benchmarking

**Last Updated**: 2025-10-29

### ðŸŽ‰ Week 7-8 Complete: Full Hybrid Architecture Integration!
- âœ… All 7 components integrated into execution path (100%)
- âœ… Event bus + LSP manager operational
- âœ… Plan/Build mode enforcement working
- âœ… Git snapshots before risky operations
- âœ… Model router for cost-aware selection
- âœ… Specialized agent selection (Explorer/Builder/Debugger)
- âœ… Research sub-agent spawning for parallel execution
- âœ… Full execution flow implemented: UserIntent â†’ Agent â†’ Model â†’ Execution
- **Ready for Week 9**: Empirical validation and benchmarking

## Week 9: Empirical Validation (Current Sprint)

### Priority 1: Integration Validation
- [ ] Review all integrated components for correctness
- [ ] Check for obvious bugs or edge cases
- [ ] Verify event flow: write_file â†’ FileChanged â†’ LSP
- [ ] Verify mode enforcement: Plan mode blocks write_file
- [ ] Verify model selection: Logs show cost-aware routing
- [ ] Verify agent selection: Logs show specialized agents chosen

**Expected Outcome**: Confidence that all components work as designed

### Priority 2: Unit/Integration Testing
- [ ] Test event bus emission and reception
- [ ] Test mode enforcement with various tool calls
- [ ] Test model router selection logic
- [ ] Test agent selection for different intents
- [ ] Test research sub-agent spawning (if possible without full LLM)
- [ ] Mock/stub tests for isolated component validation

**Expected Outcome**: Automated tests validate core integration logic

### Priority 3: Real-World Testing Strategy
- [ ] Determine if we can test locally with proper cleanup
- [ ] Consider container-based testing for isolation
- [ ] Create test scenarios for each agent type
- [ ] Test with actual Ollama models (free, local)
- [ ] Document test setup and teardown procedures

**Expected Outcome**: Safe, repeatable testing methodology

### Priority 4: Performance Measurements (if feasible)
- [ ] Measure model selection overhead
- [ ] Measure event bus latency
- [ ] Track token usage per model type
- [ ] Compare cost with/without router
- [ ] Measure sub-agent spawn time

**Expected Outcome**: Quantitative validation of hybrid architecture benefits

### Completed Weeks (Timeline)

**âœ… Week 1: File Operations (Oct 20-26)**
- 4 production tools: read_file, write_file, edit_file, list_files
- 2,110+ lines of code, 21+ tests
- Competitive parity: 17-21% â†’ 23-27%

**âœ… Week 2: Code Understanding (Skipped - tools already exist)**
- Existing tools validated: search_code, analyze_code, find_refs, find_def

**âœ… Week 3: Episodic Memory (DuckDB)**
- 5 tables: tool_executions, file_interactions, task_history, context_snapshots, learned_patterns
- 11 CRUD operations, 7 query methods
- +815 lines production code

**âœ… Week 4: Knowledge Graph (petgraph)**
- Tree-sitter extraction for Rust (expandable to 19+ languages)
- 5 node types, 6 edge types
- Binary persistence with bincode
- 8 query methods, incremental updates
- +1,470 lines production code

**âœ… Week 5: Working Memory (Dynamic Context)**
- ContextWindow with intelligent pruning (80% â†’ 30% removal)
- Relevance scoring: time_decay Ã— task Ã— dependencies Ã— type
- DynamicContextManager integrating all 3 systems
- +820 lines production code, +620 lines tests
- 9 unit tests + 8 integration tests

**âœ… Week 6 Day 1-6: ACP + Architecture Redesign**
- ACP protocol enhanced (+635 lines)
- SOTA research (Factory Droid, OpenCode, Claude Code, Amp)
- New system design created (ai/SYSTEM_DESIGN_2025.md)
- Hybrid architecture combining best patterns

## Week 8: Specialized Agents + Sub-Agents

### Day 1-2: Agent Configurations
- [ ] Create AgentConfig struct (agent_type, system_prompt, allowed_tools, max_steps)
- [ ] Define Explorer agent (CodeReading tasks)
  - System prompt: "You are a code explorer. Your goal is to understand code, find patterns, and explain functionality."
  - Tools: grep, read, glob, LSP, knowledge_graph_query
- [ ] Define Builder agent (CodeWriting tasks)
  - System prompt: "You are a code builder. Implement features precisely, follow existing patterns."
  - Tools: all tools including write, edit
- [ ] Define Debugger agent (ProjectFixing tasks)
  - System prompt: "You are a debugger. Find root causes, fix bugs systematically."
  - Tools: all tools + test runner
- [ ] Define Refactorer agent (code improvements)
  - System prompt: "You are a refactorer. Improve code while maintaining behavior."
  - Tools: read, write, edit, test runner, LSP
- [ ] Test: Each agent type with specialized task

**Expected Outcome**: Specialized agents with focused, effective prompts

### Day 3-4: Research Sub-Agents
- [ ] Implement SubAgent::spawn() for parallel research
- [ ] Task decomposition: break research query into subtasks
- [ ] Max 10 concurrent sub-agents (Claude Code limit)
- [ ] Result aggregation in main agent context
- [ ] Memory integration: check episodic memory before spawning
- [ ] Prevent duplicate research (cache hit = skip sub-agent)
- [ ] Test: "Find all auth patterns" â†’ 5 parallel sub-agents search directories

**Expected Outcome**: 90% speedup for research tasks, memory prevents duplicates

### Day 5-7: Integration Testing
- [ ] Test Plan mode with research sub-agents
  - Verify: Read-only tools, can spawn sub-agents
  - Measure: Research task completion time vs baseline
- [ ] Test Build mode NEVER uses sub-agents
  - Verify: 0% sub-agent usage for coding tasks
  - Measure: No 15x token waste
- [ ] End-to-end workflow tests
  - Start in Plan â†’ research with sub-agents â†’ transition to Build â†’ implement
- [ ] Performance benchmarks
  - Tool calls: target 60% reduction
  - Research speed: target 90% improvement
  - Token usage: confirm no sub-agents for coding

**Expected Outcome**: Validated 90% research improvement, 0% coding sub-agents

## Week 9: Benchmarks vs Claude Code

**Goal**: Validate 60% improvement from memory systems

**Benchmark Tasks**:
1. Multi-file refactoring (measure: tool calls, context efficiency)
2. Bug fixing workflow (measure: time to resolution, relevant files)
3. New feature implementation (measure: code consistency, iterations)
4. Codebase exploration (measure: irrelevant files examined)

**Metrics to Track**:
- Tool calls needed (target: 7.5 â†’ 3.0 = 60% reduction)
- Files examined (target: 7.5 â†’ 3.0 = 60% reduction)
- Irrelevant files (target: 3.5 â†’ 0.0 = 100% reduction)
- Success rate (target: maintain 100%)
- Continuous work capability (no restart needed)

**Validation Plan**:
1. Run same 4 tasks from Python POC in Rust
2. Compare Aircher (with memory) vs Claude Code (no memory)
3. Document: tool call traces, context window usage, memory queries
4. Create graphs and tables for research paper

### Week 9-10: Research Paper + Release

**Paper Outline**:
1. Introduction: Problem with context-limited coding agents
2. Related Work: ReAct, Reflexion, sub-agents, RAG
3. Architecture: Three-layer memory system
   - Episodic memory (DuckDB): Track everything, learn patterns
   - Knowledge graph (petgraph): Instant codebase queries
   - Working memory (dynamic context): Intelligent pruning
4. Evaluation: 60% reduction in tool calls, continuous work capability
5. Results: Benchmark comparisons, ablation studies
6. Discussion: When memory helps, when it doesn't
7. Conclusion: Memory-augmented agents are future

**Release Checklist**:
- [ ] Finalize documentation (README, docs/, guides)
- [ ] Create video demos (30s teaser, 5min walkthrough)
- [ ] Write blog post series (4-5 posts)
- [ ] Submit research paper (arXiv or conference)
- [ ] GitHub release (v0.1.0 with release notes)
- [ ] Social media announcement (Reddit, HN, Twitter)
- [ ] Submit to aggregators (awesome-ai-agents, etc.)

### Backlog (Post-Week 10)

**Performance Optimizations**:
- [ ] Profile memory system queries (DuckDB, petgraph)
- [ ] Optimize relevance scoring (vectorize calculations)
- [ ] Add caching layer for frequent queries
- [ ] Benchmark pruning algorithm efficiency

**Advanced Features**:
- [ ] Multi-user sessions (team collaboration)
- [ ] Cross-project pattern learning
- [ ] Export/import knowledge graphs
- [ ] Visual graph explorer (web UI)

**Tool Improvements**:
- [ ] LM-centric interfaces (windowing, limits, validation)
- [ ] Error guardrails (linting, auto-reject bad edits)
- [ ] Context management (last 5 interactions, collapse older)
- [ ] Result limits (max 50 results per query)

**Integration**:
- [ ] Neovim plugin testing
- [ ] Emacs integration testing
- [ ] JetBrains collaboration (when ready)
- [ ] VSCode via ACP adapter

## Daily Focus (2025-10-27)

**Completed**:
- âœ… Week 5 complete (all 3 memory systems + validation tests)
- âœ… Week 6 Day 1 (ACP protocol review + documentation)

**Current Status**:
- Week 6 Day 2: Ready to enhance session management

**Immediate Next**:
1. Implement session state tracking (HashMap<SessionId, SessionState>)
2. Add conversation history per session
3. Begin streaming response support

## Notes

**Major Timeline Win**: ACP protocol already 90% complete!
- Expected: 1 week to implement ACP from scratch
- Actual: Already done, just needs enhancements
- Time saved: 4-5 days (can focus on testing and polish)

**Memory System Status**: ALL 3 COMPLETE âœ…
- Week 3: Episodic Memory (+815 lines)
- Week 4: Knowledge Graph (+1,470 lines)
- Week 5: Working Memory (+820 lines) + Integration tests (+620 lines)
- **Total**: 3,725 lines of production-ready memory architecture

**Competitive Position**: 30-33% parity with Claude Code
- Infrastructure: Strong (semantic search, multi-provider, tools)
- Intelligence: Built but needs wiring (memory systems ready)
- ACP: 90% complete, ready for multi-frontend support
- Innovation: 60% improvement validated (memory systems)
