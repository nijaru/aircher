# RESEARCH

*Index of external research findings and their application to Aircher*

## SOTA Agent Architectures (researched 2025-10-27, VERIFIED 2025-10-29) ‚ú®

**VERIFICATION COMPLETE**: Comprehensive research from public sources (docs, HN, blogs, benchmarks)

**Evidence Levels**:
- ‚úÖ VERIFIED: Open source code/docs inspected with implementation details
- üìÑ DOCUMENTED: Official docs/blogs exist with details
- ‚ö†Ô∏è INFERRED: User reports, consistent patterns across sources
- ‚ùì UNKNOWN: Pure speculation, no public info

**Sources** (all verified):
- **OpenCode** - ‚úÖ github.com/sst/opencode (28.8k stars, open source)
- **Claude Code** - üìÑ Anthropic blogs + ‚ö†Ô∏è extensive user feedback (HN, Reddit)
- **Factory Droid** - ‚úÖ Terminal-Bench leaderboard (#2 at 58.8%)
- **Cursor** - üìÑ Official blogs with technical details
- **Windsurf** - üìÑ Official docs + marketing data
- **Benchmarks** - ‚úÖ SWE-bench, Terminal-Bench official leaderboards

**Verified Patterns**:

1. **Plan/Build Separation** (OpenCode) ‚úÖ VERIFIED
   - Docs: https://opencode.ai/docs/modes/ (confirmed)
   - Plan mode disables: write, edit, patch, bash tools
   - Build mode: all tools enabled (default)
   - Switch with Tab key or Ctrl+x + a
   - **VERIFIED**: Open source at github.com/sst/opencode

2. **LSP Integration** (OpenCode) ‚úÖ VERIFIED
   - Confirmed in GitHub issues and docs
   - Configurable via opencode.json
   - Language-specific LSP servers (rust-analyzer, pyright, gopls, etc.)
   - **IMPLEMENTED**: Week 7 Day 1-2 in Aircher

3. **Git Snapshots** (OpenCode) ‚úÖ VERIFIED
   - Confirmed in GitHub issues (users report problems with 98GB snapshots!)
   - Creates temporary Git commits before risky operations
   - Can cause issues with large repos/binaries
   - **IMPLEMENTED**: Week 7 Day 5 in Aircher

4. **Sub-Agents** (Claude Code) üìÑ DOCUMENTED
   - Official Anthropic blog mentions parallelization feature
   - HN discussion: "How to use Claude Code subagents to parallelize development" (288 points)
   - **VERIFIED pain points**: Users report hidden token costs, 5-hour limits
   - Terminal-Bench: 43.2% ¬± 1.3 (official score)
   - **FINDING**: Grep-only retrieval (40% more tokens than vector search)

5. **Specialized Agents** (Factory Droid) ‚úÖ VERIFIED PERFORMANCE
   - Terminal-Bench: 58.8% ¬± 0.9 (Claude Opus 4.1) - **#2 position**
   - Ante overtook at 60.3% ¬± 1.1 (now #1)
   - Architecture unknown (closed source), but results are real
   - **TARGET**: Beat 58.8% to claim SOTA

6. **Fast-Apply Models** (Cursor) üìÑ DOCUMENTED
   - Llama-3-70 fast-apply model: 1000 tokens/second
   - Sub-100ms latency for completions
   - Official blog: "Instant Apply" technical deep-dive
   - Context-aware: Embedding ‚Üí Rerank ‚Üí Apply pipeline

7. **Flow Awareness** (Windsurf Cascade) üìÑ DOCUMENTED
   - Tracks file edits, terminal commands, clipboard, conversation
   - 90% of code per user written by Cascade (official claim)
   - 57M lines generated per day (official data)
   - Proprietary models built for timeline ingestion

**Benchmark SOTA** (verified):
- **SWE-bench**: Grok 4 (75%), GPT-5 (74.9%), Opus 4.1 (74.5%)
- **Terminal-Bench**: Ante (60.3%), Factory Droid (58.8%), Claude Code (43.2%)

**Application to Aircher**:
- ‚úÖ Plan/Build modes (Week 7 Day 3-4) - IMPLEMENTED
- ‚úÖ LSP integration (Week 7 Day 1-2) - IMPLEMENTED
- ‚úÖ Git snapshots (Week 7 Day 5) - IMPLEMENTED
- ‚è≥ Model router (Week 7 Day 6-7) - PENDING
- ‚è≥ Specialized agents (Week 8 Day 1-2) - PENDING
- ‚è≥ Research sub-agents (Week 8 Day 3-4) - PENDING

**Critical Next Steps**:
1. ‚ùó Run Terminal-Bench evaluation (get baseline score)
2. ‚ùó Run SWE-bench Verified (compare vs 75% SOTA)
3. ‚ö†Ô∏è Finish Week 7-8 implementation
4. üìä Document evidence-based performance claims

**Details**: ‚Üí ai/research/competitive-analysis-2025.md (comprehensive 574-line analysis)

## Agent Scaffolding (researched 2025-10-27)

**Key Finding**: Simple scaffolding >> Complex orchestration

**Sources**:
- SWE-Agent (May 2024): 3-5x improvement from interface design alone
- SWE-Bench Pro (Sept 2024): Frontier models drop from 70% to 23% on unseen code
- Episodic Memory paper (Feb 2025): Memory improves agents 25-40%

**Application**:
- LM-centric tool interfaces (windowing, limits, validation)
- Error guardrails (linting, auto-reject)
- Episodic + semantic memory integration

**Details**: ‚Üí ai/research/agent-scaffolding.md

## Memory Systems (researched 2025-10-27)

**Key Finding**: Episodic memory is critical for long-term agents

**5 Essential Properties**:
1. Single-shot learning (learn from one experience)
2. Instance-specific contexts (remember situations)
3. Temporal organization (sequence matters)
4. Flexible retrieval (find relevant past)
5. Adaptive reuse (apply learnings)

**Real-World Evidence**: Devin's success (13.86% SWE-bench) from persistent learning

**Application**:
- DuckDB for pattern storage (already built)
- Repository auto-scanning (like Devin)
- Cross-session memory retrieval

**Details**: ‚Üí ai/research/memory-systems.md

## Architecture Insights (researched 2024-2025)

**Key Finding**: Models do reasoning internally, agents do execution externally

**What Models Handle**:
- Reasoning, planning, decisions (via prompting)
- Code generation, understanding context
- Self-reflection, pattern recognition

**What Agents Handle**:
- Tool execution (models can't run commands)
- Error prevention (guardrails, validation)
- Memory persistence (models forget)
- Interface design (LM-friendly tools)

**Evidence**: Removed 1685-line MultiTurnReasoningEngine, replaced with 300-line enhanced prompting

**Details**: ‚Üí docs/architecture/MODEL_VS_AGENT_ARCHITECTURE.md (existing)

## Competitive Intelligence (2024-2025)

**Key Finding**: Sub-agents cause 19% performance degradation vs dynamic context

**Claude Code**:
- Sub-agent architecture issues
- Rate limit problems (50+ incidents/month)
- No persistent memory across sessions

**Devin**:
- Knowledge base across sessions
- Auto-suggests learnings
- Repository scanning

**Cursor**:
- Complex UI (4+ Accept buttons)
- Model flexibility (OpenAI, Anthropic, local)

**Our Advantage**:
- Memory systems (episodic + semantic)
- Dynamic context management
- LM-centric interfaces

**Details**: ‚Üí ai/research/competitive-intelligence.md

## Research-Based Strategy (2024-2025)

**ReAct**: Thought‚ÜíAction‚ÜíObservation loop (25% improvement)
**Reflexion**: Self-reflection + episodic memory (88% vs 67%)
**Tree of Thoughts**: Multi-path exploration (70% improvement)

**Application**: Enhanced prompting system (not external orchestration)

**Details**: ‚Üí docs/architecture/ (existing)

## Knowledge Graphs for Code Understanding (researched 2025-10-27)

**Key Finding**: Knowledge graphs solve RAG limitations for code repositories

**Problem with RAG:**
- Treats code as flat documents
- Loses inter-file relationships
- Can't answer "How many functions in file X?" or "Where is variable Y used?"

**Knowledge Graph Solution:**
- Nodes: files, classes, functions, variables
- Edges: contains, calls, imports, inherits, uses
- Enables structure-aware queries

**Recent Research (2024-2025):**

**CodexGraph (Microsoft, Aug 2024)**
- Graph database with static analysis extraction
- LLM agent constructs graph queries to find relevant code
- Finding: "Static analysis + graph DB > semantic search alone"

**RepoGraph (Oct 2024)**
- Integrates line-level, file-level, repository-level context
- Surpasses flat RAG approaches
- Key: Multi-level context integration

**KGCompass (Mar 2025)**
- Links repository artifacts (issues, PRs) with code entities
- Narrows search space to 20 most relevant functions
- Application: Bug fixing with historical context

**Knowledge Graph-Based Codegen (May 2025)**
- Semantic search on code docs + structure
- Generated code aligns with project structure
- Uses functional similarity for retrieval

**Application to Aircher:**
- Build graph: tree-sitter ‚Üí extract nodes/edges ‚Üí NetworkX
- Store in DuckDB for persistence
- Query before tool execution: "What's relevant to this task?"
- Combine with episodic memory for learned patterns

**Details**: ‚Üí poc-memory-agent/README.md

## Toad TUI Frontend (researched 2025-10-29)

**Key Finding**: Professional terminal UI frontend built by expert, perfect for ACP-native agents

**Creator**: Will McGugan (Rich/Textual Python libraries, 5+ years terminal UI expertise)

**Status**: Private preview ($5,000/month sponsorship), will become open source

**Core Advantages**:
- **No flicker**: Partial character updates (vs full screen rewrites in Node tools)
- **Interactive scrollback**: Can interact with and copy previous output
- **Jank-free**: Solves terminal rendering problems from 5+ years of Textual optimization
- **Universal**: Provider-agnostic, works with any AI backend

**Features Verified**:
1. Advanced text input (multi-line, cut/copy/paste, undo/redo)
2. Markdown highlighting in real-time
3. Code fence editing with language-specific syntax highlighting
4. Shell mode (triggered by "!" or "$" prefix)
5. Smart indentation and LSP support (planned for code fences)

**Technical Foundation**:
- Built on Textual (Python TUI framework)
- Installation: `uvx toad` (similar UX to `npx`)
- Positioned as "universal UI" for agentic coding

**ACP Integration**: ‚ö†Ô∏è INFERRED (not explicitly documented, but likely given "universal" positioning)

**Application to Aircher**:
- Saves 4-6 weeks of custom TUI development
- Professional quality from terminal rendering expert
- Aligns perfectly with ACP-native architecture
- Multi-frontend strategy: Toad + Zed + Neovim + Emacs + JetBrains

**Strategy**:
1. Build Aircher as ACP-native backend (current)
2. Test with existing frontends (Zed, Neovim, Emacs)
3. Integrate Toad when open source release available
4. Position: "Best agent intelligence, works in your favorite frontend"

**Details**: ‚Üí ai/research/toad-tui-features.md (comprehensive analysis)

## Benchmarking Strategy (researched 2025-10-29)

**Key Finding**: Two industry-standard benchmarks available for empirical validation

**Terminal-Bench** (Primary Target):
- Terminal-specific agent benchmark with public leaderboard
- T-Bench-Core-v0: 80 terminal tasks
- Current SOTA: Ante (60.3%), Factory Droid (58.8%), Claude Code (43.2%)
- Target: Beat Factory Droid's 58.8% to claim SOTA

**SWE-bench** (Industry Standard):
- Most widely recognized coding agent benchmark
- Multiple datasets: Full (2,294), Verified (500), Lite (300), Multimodal (517), Live (1,565)
- Current SOTA: Grok 4 (75%), GPT-5 (74.9%), Claude Opus 4.1 (74.5%)
- Recommended: Start with SWE-bench Verified (500 tasks, human-filtered)

**Terminal-Bench Registry**:
- Unified harness for multiple benchmarks
- Can run: SWE-bench Verified, AppWorld, DevEval, EvoEval
- Single CLI interface for all evaluations

**Integration Approach**:
- ACP-based integration (tests production interface)
- Week 9 Days 1-3: Terminal-Bench baseline run
- Week 9 Days 4-5: SWE-bench Verified sample run
- Week 9 Days 6-7: Analysis and optimization planning

**Expected Baseline** (conservative):
- Terminal-Bench: 35-45% (between Claude Code and Factory Droid)
- SWE-bench Verified: 25-35%

**Optimistic Targets** (after optimization):
- Terminal-Bench: 50-60% (competitive with Factory Droid)
- SWE-bench Verified: 40-50%

**Success Criteria for Paper**:
- Minimum: >43.2% Terminal-Bench (beat Claude Code)
- Competitive: >50% Terminal-Bench (approach Factory Droid)
- SOTA: >58.8% Terminal-Bench (beat Factory Droid)

**Details**: ‚Üí ai/research/benchmark-integration-plan.md (comprehensive implementation plan)

## Open Questions

- [ ] Optimal context window size for code (100 lines? 200?)
- [ ] Best linting strategy (tree-sitter? language-specific?)
- [ ] Memory retrieval ranking (semantic similarity? recency? success rate?)
- [ ] Repository scanning patterns (what to extract?)

## Recent Discoveries

**2025-10-27**: LM-centric interfaces critical
- Our tools return too much context (should window to 100 lines)
- Missing validation/linting (should auto-reject syntax errors)
- No result limits (should max 50 results)

**Action Items**: Retrofit Week 1 tools with research patterns in Week 2

---

**Research Principle**: Always research current SOTA, don't rely on stale patterns. This index points to detailed research files for deep dives.
