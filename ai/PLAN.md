# Aircher Implementation Plan: Python + LangGraph Strategy

**Last Updated**: 2025-11-12
**Decision**: Python-only with LangGraph framework
**Timeline**: 4-6 weeks to SOTA implementation
**Target**: Terminal-Bench >58.8% (beat Factory Droid)

## Strategic Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Frontend (Any ACP-Compatible)            ‚îÇ
‚îÇ         Toad | Zed | Neovim | Emacs | VSCode     ‚îÇ
‚îÇ         (When Toad releases, primary frontend)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  ACP Protocol (stdio)  ‚îÇ
        ‚îÇ    JSON-RPC messages   ‚îÇ
        ‚îÇ  Language-agnostic!    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Aircher Agent (Python 3.13+)             ‚îÇ
‚îÇ         LangGraph + Modern Python Stack          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Memory Systems: 3-layer architecture           ‚îÇ
‚îÇ   - DuckDB: Episodic memory (60% improvement)  ‚îÇ
‚îÇ   - ChromaDB: Vector search (semantic code)    ‚îÇ
‚îÇ   - Knowledge Graph: Code structure (3,942+)   ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ LangGraph Workflow: State-based agent          ‚îÇ
‚îÇ   - Intent classification                       ‚îÇ
‚îÇ   - Tool execution with approval               ‚îÇ
‚îÇ   - Dynamic context management                 ‚îÇ
‚îÇ   - Sub-agent coordination                     ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ Tool Framework: Modern CLI tools               ‚îÇ
‚îÇ   - Bundled: ripgrep + ast-grep                ‚îÇ
‚îÇ   - Assumed: fd, jq, sd, git (with fallbacks)  ‚îÇ
‚îÇ   - Python: tree-sitter, PyYAML, toml          ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ Multi-Provider: OpenAI, Anthropic, local       ‚îÇ
‚îÇ   - Smart model routing (40% cost savings)     ‚îÇ
‚îÇ   - Ollama for unlimited local usage           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Current State Assessment

### ‚úÖ Completed (Phase 1-2)

**Python Project Setup**:
- Modern tooling: uv, ruff, ty, vulture, pytest
- Dependencies configured: LangGraph, ChromaDB, DuckDB, tree-sitter
- CI/CD: GitHub Actions with multi-Python (3.13, 3.14) testing
- Project structure: Clean separation (agent/, memory/, protocol/, tools/)
- CLI: Basic `aircher status` command working
- Tests: Basic unit tests passing

**Research & Architecture**:
- 15 research files, 5,155+ lines of SOTA analysis
- Memory system architecture fully designed (60% tool reduction validated)
- Sub-agent patterns researched (Crush analysis)
- Competitive analysis complete (OpenCode, Claude Code, Factory Droid)
- Benchmarking strategy documented (Terminal-Bench, SWE-bench)
- Strategic decisions documented (Python, multi-DB, READ/WRITE modes)

**Tool Framework**:
- Tool manager with bundling system (`src/aircher/tools/manager.py`)
- Bash wrapper for shell execution
- File operations (read, write, edit basics)
- Platform detection and download logic

### ‚ùå Critical Gaps (Phase 3-4)

**Memory Systems** (Fully Designed, NOT Implemented):
- ‚ùå DuckDB episodic memory (schema specified, queries designed)
- ‚ùå ChromaDB vector search (dependency installed, not integrated)
- ‚ùå Knowledge graph (tree-sitter extraction not ported from POC)
- **Research**: 667 lines of detailed architecture in ai/research/memory-system-architecture.md
- **Implementation**: 0 lines (empty `memory/__init__.py`)

**LangGraph Agent** (Skeleton Only):
- ‚ö†Ô∏è State definition exists (50 lines)
- ‚ùå Workflow graph: TODOs only
- ‚ùå Tool integration missing
- ‚ùå Memory integration missing
- ‚ùå Sub-agent support missing

**Sub-Agent System** (Patterns Researched, NOT Implemented):
- ‚ùå Specialized agent routing (CodeReading, CodeWriting, ProjectFixing)
- ‚ùå Parallel execution capabilities
- ‚ùå Tool restriction per agent type (Crush patterns)
- **Research**: 425 lines from ai/research/crush-subagent-architecture.md
- **Implementation**: 0 lines

**Dynamic Context Management** (Algorithm Designed, NOT Implemented):
- ‚ùå Relevance scoring (time decay, task association, dependencies)
- ‚ùå Intelligent pruning (remove bottom 30% by relevance)
- ‚ùå Context snapshot tracking for recovery
- **Research**: 300+ lines of algorithm specification
- **Implementation**: 0 lines

## 4-6 Week Implementation Plan

### Week 1: Memory Systems Foundation

**DuckDB Episodic Memory** (Days 1-4):
```sql
-- Implement schema from ai/research/memory-system-architecture.md
CREATE TABLE tool_executions (
    id INTEGER PRIMARY KEY,
    timestamp TIMESTAMP NOT NULL,
    session_id VARCHAR NOT NULL,
    tool_name VARCHAR NOT NULL,
    parameters JSON NOT NULL,
    result JSON,
    success BOOLEAN NOT NULL,
    duration_ms INTEGER,
    context_tokens INTEGER
);

CREATE TABLE file_interactions (
    id INTEGER PRIMARY KEY,
    timestamp TIMESTAMP NOT NULL,
    session_id VARCHAR NOT NULL,
    file_path VARCHAR NOT NULL,
    operation VARCHAR NOT NULL,
    success BOOLEAN NOT NULL,
    context TEXT
);

CREATE TABLE learned_patterns (
    id INTEGER PRIMARY KEY,
    pattern_type VARCHAR NOT NULL,
    pattern_data JSON NOT NULL,
    confidence FLOAT NOT NULL,
    observed_count INTEGER NOT NULL
);
```

**Implementation**:
- `src/aircher/memory/duckdb_memory.py` - Connection, schema, basic queries
- `src/aircher/memory/episodic.py` - High-level API for tracking
- Hook into tool execution to record all operations
- Query interface: "Have I seen this before?", "Co-edit patterns"

**ChromaDB Vector Search** (Days 5-7):
- Initialize embedding model (sentence-transformers)
- Create collection for code snippets
- Index codebase on startup (async background task)
- Query interface for semantic code search
- Integration with LangGraph agent workflow

**Success Criteria**:
- All tables created and queries working
- Tool executions automatically recorded
- Semantic search returns relevant code snippets
- POC 60% improvement mechanism operational

### Week 2: Knowledge Graph & LangGraph Integration

**Knowledge Graph** (Days 1-3):
- Port tree-sitter extraction from POC (`poc-memory-agent/`)
- Build graph structure: files ‚Üí classes ‚Üí functions
- Edges: contains, calls, imports, inherits
- Serialize to DuckDB for persistence
- Query interface: "What's in file X?", "What calls Y?"

**LangGraph Agent Workflow** (Days 4-7):
- Complete workflow graph definition
- Integrate tools (file ops, bash, search)
- Add memory system queries
- Implement intent classification node
- Add permission/approval nodes for WRITE mode
- Test end-to-end: user input ‚Üí tool execution ‚Üí response

**Success Criteria**:
- Knowledge graph builds on startup
- LangGraph workflow executes complete cycles
- Tools integrated and working
- Memory queries inform agent decisions

### Week 3: Sub-Agent System & Context Management

**Sub-Agent Architecture** (Days 1-3):
- Implement specialized agents (Crush patterns):
  - CodeReading: File analysis (READ mode tools only)
  - CodeWriting: Edits and refactoring (WRITE mode tools)
  - ProjectFixing: Bug fixing and testing (full tools)
  - ResearchAgent: Documentation, web search (external tools)
- Tool restriction per agent type
- Session hierarchy (parent/child tracking)
- Cost optimization (small models for sub-agents)

**Dynamic Context Management** (Days 4-7):
- Implement relevance scoring algorithm (from memory-system-architecture.md):
  ```python
  score = time_score * task_boost * dependency_boost * type_multiplier * relevance
  ```
- Intelligent pruning (remove bottom 30% by relevance when at 80% capacity)
- Context snapshot to episodic memory before pruning
- Prefetch relevant code from knowledge graph
- Test continuous work without context restarts

**Success Criteria**:
- Sub-agents spawn and execute correctly
- Tool restrictions enforced per agent type
- Context pruning prevents overflow
- Continuous multi-turn conversations work

### Week 4: Model Routing & Cost Optimization

**Smart Model Router** (Days 1-3):
- Implement model selection logic:
  - Large/expensive (Opus, GPT-4): Main agent reasoning
  - Small/cheap (Haiku, GPT-4o-mini): Sub-agents, simple tasks
  - Local (Ollama): Unlimited usage, development
- Cost tracking per session
- Automatic fallback on rate limits
- User-configurable model preferences

**LLM Provider Integration** (Days 4-7):
- Complete OpenAI integration (GPT-4, GPT-4o-mini)
- Complete Anthropic integration (Opus, Sonnet, Haiku)
- Ollama integration for local models
- Streaming response support
- Error handling and retries

**Success Criteria**:
- Model router selects appropriately
- Cost savings validated (target 40%)
- All providers working
- Graceful fallback on failures

### Week 5: ACP Protocol & Testing

**ACP Protocol Implementation** (Days 1-3):
- stdio transport (JSON-RPC over stdin/stdout)
- Session management (create, resume, end)
- Tool execution via protocol
- Streaming responses
- Reference: https://agentclientprotocol.com/

**Comprehensive Testing** (Days 4-7):
- Unit tests for all memory operations
- Integration tests for agent workflow
- Tool execution tests with approval mocking
- Memory persistence tests
- Performance benchmarks (response time, memory usage)

**Success Criteria**:
- ACP protocol compliant
- Can launch from Zed or other ACP clients
- Test coverage >80%
- Performance targets met (<100ms p95 for simple queries)

### Week 6: Benchmarking & Optimization

**Terminal-Bench Integration** (Days 1-3):
- Set up Terminal-Bench evaluation harness
- Run baseline evaluation (expect 35-45% initially)
- Identify failure patterns
- Optimize based on findings

**SWE-bench Sample Run** (Days 4-5):
- Run SWE-bench Verified sample (50 tasks)
- Analyze performance
- Compare with SOTA (75% current best)

**Optimization & Polish** (Days 6-7):
- Address benchmark failures
- Tune prompts and workflows
- Optimize memory queries
- Validate 60% tool reduction claim
- Final testing and bug fixes

**Success Criteria**:
- Terminal-Bench: >43.2% (beat Claude Code baseline)
- Competitive target: >50% (approach Factory Droid 58.8%)
- Stretch goal: >58.8% (beat Factory Droid, claim SOTA)
- Tool call reduction validated

## Technical Stack

### Core Dependencies
```toml
# Agent Framework
langgraph>=0.2.0          # State-based agent workflows
langchain>=0.3.0          # LLM abstraction layer
langchain-openai>=0.2.0   # OpenAI integration
langchain-anthropic>=0.2.0 # Anthropic integration

# Memory Systems
duckdb>=0.10.0            # Episodic memory, analytics
chromadb>=0.5.0           # Vector search, embeddings
sentence-transformers     # Embedding model

# Code Analysis
tree-sitter>=0.21.0       # Multi-language parsing
tree-sitter-python        # Python parsing
tree-sitter-rust          # Rust parsing
tree-sitter-javascript    # JavaScript parsing

# CLI & Async
click>=8.1.0              # CLI framework
rich>=13.7.0              # Terminal formatting
aiohttp>=3.9.0            # Async HTTP
aiofiles>=24.0.0          # Async file I/O

# Development
pytest>=8.0.0             # Testing framework
ruff>=0.6.0               # Linting and formatting
vulture>=2.10             # Dead code detection
```

### Bundled Tools (Zero Setup)
- **ripgrep**: Fast code search (10-100x faster than grep)
- **ast-grep**: Semantic code patterns (AST-aware queries)
- Total size: ~15MB, downloaded to `~/.aircher/tools/bin`

### Assumed Tools (with Fallbacks)
- **fd ‚Üí find**: Fast file discovery
- **jq ‚Üí Python json**: JSON processing
- **sd ‚Üí sed**: Stream editing
- **git**: Required (no fallback)

## Architecture Decisions

### Why Python Over Rust?
(From ai/DECISIONS.md, 2025-11-12)

**Rationale**:
- Development velocity: 3-5x faster for agent development
- LangGraph: Production-validated framework, built for agents
- Rich AI/ML ecosystem: sentence-transformers, ChromaDB, DuckDB
- Strong Python skills vs learning Rust async patterns

**Tradeoffs**:
- ‚úÖ Faster development, rich ecosystem, easier testing
- ‚ùå Lower raw performance, GIL limitations, memory overhead
- **Mitigation**: Mojo for performance-critical components (Phase 7+)

### Why Multi-Database Strategy?
(From ai/DECISIONS.md, 2025-11-12)

**Rationale**:
- SQLite: Sessions (ACID, embedded, proven)
- DuckDB: Analytics, episodic memory (columnar, fast queries)
- ChromaDB: Vector search (specialized, embedding support)

**Tradeoffs**:
- ‚úÖ Right tool for each job, optimized performance
- ‚ùå Complexity of 3 systems, synchronization overhead
- **Mitigation**: Clean abstractions, async operations

### Why Custom ACP Implementation?
(From ai/DECISIONS.md, 2025-11-12)

**Rationale**:
- agent-protocol package conflicts with pydantic v2
- Full control over features and extensions
- Better integration with our architecture

**Tradeoffs**:
- ‚úÖ No dependency conflicts, custom extensions
- ‚ùå More implementation work, maintenance responsibility
- **Mitigation**: Follow spec closely, comprehensive tests

## Competitive Positioning

### Our Unique Advantages

**Memory-Augmented Architecture**:
- 3-layer memory: Episodic + Semantic + Knowledge Graph
- 60% tool call reduction (validated in POC)
- Cross-session learning and pattern recognition
- **Competitors**: Claude Code has no persistent memory

**Dynamic Context Management**:
- Intelligent pruning with relevance scoring
- Continuous work without context restarts
- Prefetch relevant code from knowledge graph
- **Competitors**: Claude Code restarts when context fills

**Local-First with Unlimited Usage**:
- Ollama integration for local models
- No rate limits with local execution
- Consistent performance (no infrastructure variability)
- **Competitors**: Claude Code limited by API rate limits

**Transparent Execution**:
- Visible reasoning and decision steps
- User control over approval workflows
- Learn from agent's decision-making
- **Competitors**: Claude Code "flying blind" experience

### Performance Targets

**Response Time**: <100ms p95 for simple queries
**Memory Usage**: <1GB for typical workloads
**Context Window**: 10K+ tokens with smart pruning
**Tool Execution**: <500ms for file operations
**Benchmarks**:
- Terminal-Bench: >43.2% (beat Claude Code)
- Target: >50% (approach Factory Droid)
- Stretch: >58.8% (beat Factory Droid, claim SOTA)

## Risk Mitigation

### Implementation Risks

**Memory System Complexity**:
- Risk: 3 databases might be over-engineered
- Mitigation: Each serves distinct purpose, clean abstractions
- Fallback: Can simplify to SQLite + ChromaDB if needed

**LangGraph Learning Curve**:
- Risk: Team unfamiliar with LangGraph patterns
- Mitigation: Excellent documentation, production examples available
- Fallback: Can fall back to simpler LangChain if needed

**Benchmark Performance**:
- Risk: May not hit >58.8% Terminal-Bench initially
- Mitigation: Systematic optimization based on failure analysis
- Fallback: Focus on unique advantages (memory, local models)

### Timeline Risks

**Optimistic: 4 weeks** (if everything goes smoothly)
**Realistic: 5-6 weeks** (account for debugging, optimization)
**Pessimistic: 8 weeks** (if major architectural issues found)

**Mitigation Strategy**:
- Week 1-2: Focus on memory (biggest advantage)
- Week 3-4: Core agent functionality
- Week 5-6: Polish and benchmarking
- Can skip benchmarking if timeline tight, focus on memory

## Success Metrics

### Technical Success
- ‚úÖ 3-layer memory operational (DuckDB + ChromaDB + Knowledge Graph)
- ‚úÖ Sub-agent system working (parallel execution, specialized routing)
- ‚úÖ Dynamic context pruning active (relevance-based, no restarts)
- ‚úÖ ACP protocol compliant (works with Zed, other clients)
- ‚úÖ Comprehensive tests (>80% coverage)

### Performance Success
- ‚úÖ Response time: <100ms p95 for simple queries
- ‚úÖ Memory usage: <1GB typical workloads
- ‚úÖ Tool call reduction: 60% improvement validated
- ‚úÖ Benchmark: >43.2% Terminal-Bench (beat Claude Code)
- üéØ Stretch: >58.8% Terminal-Bench (beat Factory Droid)

### Competitive Success
- ‚úÖ Clear differentiation: Memory + local + transparent
- ‚úÖ User pain points addressed: No rate limits, visible reasoning
- ‚úÖ SOTA features: All research findings implemented
- ‚úÖ Empirical validation: Benchmarks run, claims backed by data

## Next Immediate Actions

1. **Create ai/design/** directory with implementation specs
2. **Implement DuckDB schema** from ai/research/memory-system-architecture.md
3. **Start ChromaDB integration** with sentence-transformers
4. **Complete LangGraph workflow** with tool integration
5. **Update TODO.md** to reflect this implementation plan

## References

- **Architecture Research**: ai/research/memory-system-architecture.md (667 lines)
- **Sub-Agent Patterns**: ai/research/crush-subagent-architecture.md (425 lines)
- **Competitive Analysis**: ai/research/competitive-analysis-2025.md (573+ lines)
- **Benchmarking Strategy**: ai/research/benchmark-integration-plan.md (483 lines)
- **Tool Strategy**: ai/research/tools-strategy.md (consolidated)
- **POC Validation**: poc-memory-agent/ (60% improvement validated)

---

**Key Insight**: We have comprehensive SOTA research and sound architecture. The gap is implementation. Follow the research systematically, implement the 3-layer memory first (biggest advantage), then build out agent workflow and sub-agents. Benchmark early and often. We can reach SOTA in 4-6 weeks if we execute this plan methodically.
