# Competitive Analysis: SOTA Coding Agents (2025)

**Last Updated**: 2025-10-29
**Purpose**: Honest assessment of what competitors do, what we can verify, and where we're guessing

## Evidence Levels

- **‚úÖ VERIFIED**: Open source code inspected, or official documentation with implementation details
- **‚ö†Ô∏è INFERRED**: Public statements, user reports, blog posts, but no code to inspect
- **‚ùì UNKNOWN**: No public information, pure speculation
- **üìÑ DOCUMENTED**: Official docs/blogs exist but implementation hidden

---

## Open Source Agents (Can Inspect Code)

### 1. OpenCode (thdxr)
**GitHub**: https://github.com/thdxr/opencode (hypothetical - need to verify)
**Evidence Level**: ‚úÖ VERIFIED (if truly open source) / ‚ö†Ô∏è INFERRED (if just blog posts)

**What We Know FOR SURE**:
- ‚úÖ Plan/Build mode separation (from blog posts/tweets)
- ‚úÖ LSP integration (from blog posts)
- ‚úÖ Git-based undo/snapshots (from blog posts)
- ‚úÖ Event bus architecture (from blog posts)

**What We NEED TO VERIFY**:
- [ ] Is the repo actually public? Check GitHub
- [ ] Can we read the actual implementation?
- [ ] Are our assumptions about the design correct?

**Action Items**:
- Find and inspect actual OpenCode source code
- If closed source, downgrade all claims to ‚ö†Ô∏è INFERRED

---

### 2. Zed AI
**GitHub**: https://github.com/zed-industries/zed
**Evidence Level**: ‚úÖ VERIFIED (open source)

**Verified Features**:
- ‚úÖ LSP integration (native, part of editor)
- ‚úÖ Multi-provider support (OpenAI, Anthropic, local)
- ‚úÖ Inline assistant
- ‚úÖ Agent Client Protocol (ACP) support

**Implementation Details**:
- Language: Rust + GPUI
- Architecture: Native LSP integration, no separate event bus
- Agent system: Basic tool calling, no complex orchestration

**Unique Patterns**:
- GPUI framework for native UI
- Direct LSP integration (no abstraction layer needed)
- ACP server/client implementation

---

### 3. Sweep
**GitHub**: https://github.com/sweepai/sweep
**Evidence Level**: ‚úÖ VERIFIED (open source)

**Verified Features**:
- ‚úÖ GitHub integration (issues ‚Üí PRs)
- ‚úÖ Planning system
- ‚úÖ Multi-step execution
- ‚úÖ Test-driven fixes

**Implementation Details**:
- Language: Python
- Architecture: Planning ‚Üí Implementation ‚Üí Testing loop
- Memory: Stores context in GitHub issues/PRs

**Unique Patterns**:
- Issue-driven workflow
- Automatic PR generation
- Test-based validation

---

## Closed Source Agents (Must Infer)

### 4. Claude Code (Anthropic)
**Evidence Level**: ‚ö†Ô∏è INFERRED (no source code, only user reports + Anthropic statements)

**What We Can Infer** (from user complaints on Reddit/HN/Twitter):
- ‚ö†Ô∏è Uses sub-agents for tasks (users report 160k tokens for 3k work)
- ‚ö†Ô∏è Sub-agents cause 15x overhead (from user complaints)
- ‚ö†Ô∏è Rate limit issues (50+ reports/month)
- ‚ö†Ô∏è No persistent memory across sessions
- ‚ö†Ô∏è No git-based undo (users request feature)

**What We DON'T Know**:
- ‚ùì Actual architecture (might not use sub-agents at all)
- ‚ùì Internal tool system
- ‚ùì Reasoning strategy
- ‚ùì Context management approach

**Sources**:
- Reddit r/ClaudeAI user complaints
- Hacker News discussions
- Anthropic blog posts (generic, no implementation details)

**HONESTY CHECK**: We're basing our entire "sub-agents are bad" claim on user reports. We don't know how Claude Code actually works.

---

### 5. Factory Droid
**Evidence Level**: ‚ö†Ô∏è INFERRED (closed source, only benchmark results + marketing)

**What We Can Infer** (from Terminal-Bench leaderboard):
- ‚ö†Ô∏è #1 on Terminal-Bench (58.8% success rate)
- ‚ö†Ô∏è Uses "specialized droids" (from marketing materials)
- ‚ö†Ô∏è Pre-configured prompts per task type (from docs)

**What We DON'T Know**:
- ‚ùì Actual architecture
- ‚ùì How "droids" are implemented
- ‚ùì Tool system design
- ‚ùì Whether it's fundamentally different or just good prompts

**Sources**:
- Terminal-Bench leaderboard
- Factory Droid website marketing
- No technical blog posts or talks

**HONESTY CHECK**: We're assuming "specialized droids" = specialized agent configs. Could just be different system prompts.

---

### 6. Sourcegraph Amp (formerly Cody)
**Evidence Level**: üìÑ DOCUMENTED (closed source, but docs exist)

**What's Documented**:
- üìÑ Multi-model routing (Haiku/Sonnet/Opus)
- üìÑ Context fetching from codebase
- üìÑ LLM-based agent system

**What We Can Infer**:
- ‚ö†Ô∏è Cost-aware model selection (from docs)
- ‚ö†Ô∏è Task complexity determines model (from docs)

**What We DON'T Know**:
- ‚ùì Implementation details of router
- ‚ùì How complexity is measured
- ‚ùì Actual cost savings

**Sources**:
- Sourcegraph docs
- Blog posts about Cody/Amp

**HONESTY CHECK**: Docs say it routes between models, but no implementation details or real cost numbers.

---

### 7. Cursor
**Evidence Level**: ‚ö†Ô∏è INFERRED (partially open, mostly closed)

**What We Can Infer**:
- ‚ö†Ô∏è Multi-model support (visible in UI)
- ‚ö†Ô∏è Inline editing
- ‚ö†Ô∏è Composer mode for agentic tasks
- ‚ö†Ô∏è Complex approval workflow (users complain about "4+ accept buttons")

**What We DON'T Know**:
- ‚ùì Agent architecture
- ‚ùì Context management strategy
- ‚ùì Memory/persistence approach

**Sources**:
- Cursor website
- User reports
- YouTube demos

**HONESTY CHECK**: Very little known about internals. Mostly UI observations.

---

### 8. Windsurf (Codeium)
**Evidence Level**: ‚ö†Ô∏è INFERRED (closed source)

**What We Can Infer**:
- ‚ö†Ô∏è "Cascade" flow for multi-step tasks
- ‚ö†Ô∏è Context-aware suggestions
- ‚ö†Ô∏è Inline editing

**What We DON'T Know**:
- ‚ùì Everything about the architecture

**Sources**:
- Windsurf website
- Marketing materials

**HONESTY CHECK**: Minimal information available.

---

### 9. Google Jules (Gemini Code Assist Agent)
**Evidence Level**: ‚ùì UNKNOWN (very limited public info)

**What We Can Infer**:
- ‚ö†Ô∏è Autonomous bug fixing (from Google I/O demo)
- ‚ö†Ô∏è GitHub integration
- ‚ö†Ô∏è Multi-step planning

**What We DON'T Know**:
- ‚ùì Everything else

**Sources**:
- Google I/O announcement
- Limited blog post

**HONESTY CHECK**: Almost no information. Mostly speculation.

---

### 10. Devin (Cognition AI)
**Evidence Level**: ‚ö†Ô∏è INFERRED (closed, but some demos/interviews)

**What We Can Infer** (from demos + SWE-bench results):
- ‚ö†Ô∏è 13.86% SWE-bench score (verified)
- ‚ö†Ô∏è Persistent memory across sessions (from demos)
- ‚ö†Ô∏è Repository scanning (from demos)
- ‚ö†Ô∏è Long-running autonomous tasks (from demos)

**What We DON'T Know**:
- ‚ùì Memory implementation
- ‚ùì Planning architecture
- ‚ùì Tool system

**Sources**:
- SWE-bench leaderboard
- Cognition AI demos
- Blog posts

---

## Editor-Based Systems

### 11. GitHub Copilot
**Evidence Level**: ‚ö†Ô∏è INFERRED (closed source)

**What We Know**:
- ‚úÖ Inline completion (verified by using it)
- ‚úÖ Chat interface (verified)
- ‚ö†Ô∏è Workspace indexing (from docs)

**What We DON'T Know**:
- ‚ùì Agent architecture (if any)
- ‚ùì Context strategy

---

## Pattern Analysis: What's Actually Common vs Unique?

### Common Patterns (Most/All Agents Have):
1. **Multi-model support** - Everyone offers multiple LLMs
2. **File operations** - Read/write/edit files
3. **Inline editing** - Editor-based agents have this
4. **Context from LSP** - Editor-based agents get this for free
5. **Basic tool calling** - Industry standard now

### Uncommon Patterns (Only Some Have):
1. **Plan/Build separation** - Only OpenCode (verified if open source)
2. **Git snapshots** - Only OpenCode (verified if open source)
3. **Sub-agents** - Claude Code (inferred from user complaints)
4. **Specialized agent configs** - Factory Droid (inferred from marketing)
5. **Persistent episodic memory** - Devin (inferred from demos), Aircher (we have it)

### Unique to Aircher:
1. **Three-layer memory** (Episodic + Knowledge Graph + Working) - No one else documented
2. **Dynamic context pruning** - No one else documented
3. **60% tool call reduction** - Our POC result, not yet validated in production

---

## What We Can Confidently Claim

### ‚úÖ VERIFIED SOTA (We Can Prove):
1. **Memory systems** - No other agent has all 3 (episodic + knowledge graph + working memory)
2. **ACP protocol** - Multi-frontend support (Zed has it, but they're an editor)
3. **Dynamic context pruning** - Unique to us (not seen elsewhere)

### ‚ö†Ô∏è INSPIRED BY (Can't Prove Superiority):
1. **Plan/Build modes** - From OpenCode (IF we can verify their implementation)
2. **LSP integration** - Common in editors, less common in CLI agents
3. **Git snapshots** - From OpenCode (IF we can verify)
4. **Model routing** - From Amp (but no cost data to compare)

### ‚ùå CANNOT CLAIM (Insufficient Evidence):
1. **"Better than Claude Code"** - We don't know how it works
2. **"Better than Factory Droid"** - It's closed source, #1 on benchmarks
3. **"Hybrid is superior"** - We haven't run comparative benchmarks yet

---

## Action Items: Research We Need to Do

### Critical (Must Do Before Claiming SOTA):
1. [ ] **Find OpenCode source code** - Is it actually open source?
2. [ ] **Run benchmarks vs Claude Code** - Prove our claims with data
3. [ ] **Test Cursor/Windsurf** - Understand their actual capabilities
4. [ ] **Analyze Zed source code** - Learn from their ACP implementation
5. [ ] **Study Sweep source code** - Understand their planning system

### Important (Should Do):
1. [ ] **Monitor SWE-bench leaderboard** - Track Factory Droid, Devin scores
2. [ ] **Collect Claude Code user feedback** - Validate sub-agent waste claims
3. [ ] **Sourcegraph Amp docs deep-dive** - Understand model routing
4. [ ] **Jules documentation** - Track Google's approach when public

### Research Questions:
1. Is LSP integration unique, or do all CLI agents have it?
2. Do other agents use event-driven architectures internally?
3. Are we the only ones with persistent memory?
4. What patterns are we missing that competitors have?

---

## Honest Assessment: Where We Stand

### What We KNOW We're Good At:
- ‚úÖ Memory systems (episodic + knowledge graph + working memory)
- ‚úÖ Rust implementation (performance advantage)
- ‚úÖ ACP protocol (multi-frontend support)

### What We THINK We're Good At (Need to Prove):
- ‚ö†Ô∏è Dynamic context management
- ‚ö†Ô∏è Tool call reduction (60% in POC)
- ‚ö†Ô∏è Continuous work without restart

### What We DON'T Know:
- ‚ùì How we compare to Claude Code (no benchmarks)
- ‚ùì How we compare to Factory Droid (#1 on Terminal-Bench)
- ‚ùì Whether our architecture is actually better
- ‚ùì If anyone else has similar patterns we don't know about

---

## Next Steps: Becoming Truly SOTA

### Week 8-9: Competitive Research
1. Find and read ALL open source agent code
2. Try ALL major competitors (Claude Code, Cursor, Windsurf)
3. Run SWE-bench tasks against them
4. Document what they actually do (not what we think they do)

### Week 9: Benchmarks
1. Run same tasks in Aircher vs Claude Code vs Cursor
2. Measure: tool calls, context efficiency, success rate, cost
3. Get empirical evidence for our claims
4. Publish honest comparison

### Week 10: Research Paper
1. Only claim what we can prove
2. Mark inference vs verification clearly
3. Focus on our unique contributions (memory systems)
4. Be honest about limitations

---

## Conclusion

**What We Got Right**:
- Memory systems are genuinely unique
- ACP support is valuable
- Rust implementation is solid

**What We Need to Fix**:
- Stop claiming patterns are from competitors without verification
- Get empirical benchmarks before claiming superiority
- Be honest: "Inspired by OpenCode" not "Proven better than Claude Code"

**Research Principle**:
> "Verify before claiming. Infer cautiously. Mark speculation clearly."
